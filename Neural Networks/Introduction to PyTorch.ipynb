{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение NumPy и PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принципы работы в PyTorch немногим отличаются от операций с массивами в NumPy. Рассмотрим следующую функцию ошибки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L=\\| y - X\\cdot \\Theta\\|^2_2+\\lambda \\|\\Theta\\|_1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данную функцию можно последовательно реализовать средствами NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.50531743467711\n"
     ]
    }
   ],
   "source": [
    "y = np.random.randn(2, 1)\n",
    "X = np.random.randn(2, 5)\n",
    "theta = np.random.randn(5, 1)\n",
    "lmbda = 0.1\n",
    "\n",
    "pred = X.dot(theta)\n",
    "pred_loss = np.sum((y - pred) ** 2)\n",
    "reg_loss = lmbda * np.sum(np.abs(theta))\n",
    "loss = pred_loss + reg_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На PyTorch реализация вычисления функции $L$ выглядит аналогично, однако вместо массивов NumPy используются специальные объекты &ndash; `torch.Tensor`. Создать тензор в PyTorch можно как из простого массива, так и из массива Numpy, при этом указав тип данных, содержащихся в тензоре (по умолчанию float32).\n",
    "\n",
    "Подробнее о типах данных в PyTorch: https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.5053, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = torch.Tensor(y)\n",
    "X = torch.Tensor(X)\n",
    "theta = torch.Tensor(theta).requires_grad_(True) # для theta понадобится вычислить градиент\n",
    "\n",
    "pred = X.matmul(theta)\n",
    "pred_loss = torch.sum((y - pred) ** 2)\n",
    "reg_loss = lmbda * torch.sum(torch.abs(theta))\n",
    "loss = pred_loss + reg_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.LongTensor\n",
      "torch.LongTensor\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor([1, 2, 3]).type())\n",
    "print(torch.Tensor(np.array([1, 2, 3])).type())\n",
    "print(torch.LongTensor([1, 2, 3]).type())\n",
    "print(torch.zeros([2, 2, 3], dtype=torch.int64).type())\n",
    "\n",
    "print(torch.Tensor([[[1, 2, 3], [4, 5, 6]],[[7, 8, 9],[10, 11, 12]]]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей градиентными методами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение моделей в PyTorch производится в цикле. Ниже приведена реализация метода градиентного спуска. Чтобы обучать модели градиентными методами необходимо помнить о том, что на каждой итерации необходимо \"обнулять градиенты\" (в примере ниже `theta.grad.zero_()`), поскольку по умолчанию PyTorch накапливает градиент, а не вычисляет его заново для текущих значений параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    pred = X.matmul(theta)\n",
    "    pred_loss = torch.sum((y - pred) ** 2)\n",
    "    reg_loss = lmbda * torch.sum(torch.abs(theta))\n",
    "    loss = pred_loss + reg_loss\n",
    "\n",
    "    loss.backward() # вычисляет градиент функции по указанным переменным при их текущих значениях\n",
    "    theta.data.add_(-0.1*theta.grad.data)\n",
    "    theta.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нижнее подчеркивание у функций перезаписывает значения переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([55., 50., 13.])\n",
      "tensor([50., 45.,  8.])\n",
      "tensor([55., 50., 13.])\n",
      "tensor([55., 50., 13.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([50, 45, 8])\n",
    "b = 5\n",
    "\n",
    "print(a.add(b))\n",
    "print(a)\n",
    "print(a.add_(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование архитектуры нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинарная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае бинарной классификации в качестве функции потерь используется, как правило, бинарная кросс-энтропия. Она реализована в PyTorch в виде двух функций: `BCELoss` и `BCEWithLogitsLoss`.\n",
    "\n",
    "Отличие данных функций состоит в том, что `BCELoss` вычисляет величину ошибки от выхода сети и требует, чтобы в архитектуре явно была указана функция активации на выходном слое. Функция `BCEWithLogitsLoss`, прежде чем вычислять величину потерь, применяет функцию активации (логистическую сигмоиду) к последнему слою, поэтому в архитектуре задавать функцию активации не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5, 3))\n",
    "y = torch.FloatTensor(np.random.randint(0, 2, 5)) # float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Архитектура с `BCELoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 1),\n",
    "    torch.nn.Sigmoid() # явно задана функция активации\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X) # выдает вероятности\n",
    "    loss = criterion(pred, y.unsqueeze(1)) # размерность pred (5,1), размерность y - (5) \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2664],\n",
      "        [0.2660],\n",
      "        [0.2667],\n",
      "        [0.2666],\n",
      "        [0.2662]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(pred) # возвращает вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример с `BCEWithLogitsLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 1) # последний слой - линейный\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y.unsqueeze(1)) # размерности для BCEWithLogitsLoss должны совпадать\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8630],\n",
      "        [-0.8760],\n",
      "        [-0.8447],\n",
      "        [-0.8424],\n",
      "        [-0.8520]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.2967],\n",
      "        [0.2940],\n",
      "        [0.3005],\n",
      "        [0.3010],\n",
      "        [0.2990]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(pred) # возвращает выход последнего лоинейного слоя\n",
    "print(torch.sigmoid(pred)) # преобразование в вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многоклассовая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарная классификация &ndash; частный случай многоклассовой классификации. Для многоклассовой классификации используется кросс-энтропия. Функция потерь `CrossEntropyLoss` также не требует указания функции активации последним слоем в архитектуре сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5, 3))\n",
    "y = torch.LongTensor(np.random.randint(0, 2, 5)) # CrossEntropyLoss требует формата int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 2) # на выходном слое 2 нейрона\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss() # применяет softmax к последнему слою и вычисляет ошибку\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6010, 0.3990],\n",
      "        [0.6051, 0.3949],\n",
      "        [0.5938, 0.4062],\n",
      "        [0.5976, 0.4024],\n",
      "        [0.5950, 0.4050]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(pred, 1)) # преобразование в вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Случай нескольких классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5, 3))\n",
    "y = torch.LongTensor(np.random.randint(0, 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 3) # 3 нейрона, соответствующие 3 классам\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5792, 0.2055, 0.2153],\n",
      "        [0.5801, 0.2050, 0.2149],\n",
      "        [0.5822, 0.2025, 0.2152],\n",
      "        [0.5780, 0.2070, 0.2150],\n",
      "        [0.5803, 0.2046, 0.2151]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(pred, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оформление архитектуры в виде класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_1 = torch.nn.Linear(3, 20)\n",
    "        self.layer_2 = torch.nn.Linear(20, 10)\n",
    "        self.layer_out = torch.nn.Linear(10, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output_1 = self.sigmoid(self.layer_1(inputs))\n",
    "        output_2 = self.sigmoid(self.layer_2(output_1))\n",
    "        output = self.layer_out(output_2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layer_1): Linear(in_features=3, out_features=20, bias=True)\n",
      "  (layer_2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (layer_out): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  1  loss:  0.8136104345321655\n",
      "iter:  2  loss:  0.708175778388977\n",
      "iter:  3  loss:  0.620456337928772\n",
      "iter:  4  loss:  0.5473036766052246\n",
      "iter:  5  loss:  0.4860546588897705\n",
      "iter:  6  loss:  0.4345078468322754\n",
      "iter:  7  loss:  0.39087146520614624\n",
      "iter:  8  loss:  0.35370010137557983\n",
      "iter:  9  loss:  0.3218329846858978\n",
      "iter:  10  loss:  0.294339120388031\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5,3))\n",
    "y = torch.FloatTensor(np.random.randint(0,2,5))\n",
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X)\n",
    "    loss = criterion(pred, y.unsqueeze(1))\n",
    "    print('iter: ', i+1,' loss: ', loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h3> Упражнение</h3>\n",
    "<p></p>\n",
    "Реализовать и обучить модели из предыдущих заданий (binary_classification.csv, multiclass_classification.csv, светофор)\n",
    " <p></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
